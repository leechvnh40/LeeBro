# 基础

## 用户态和内核态

参考文章：[InfoQ 文章](https://xie.infoq.cn/article/25df22c38dc0e925879ce4e9b)

简单来说就是操作系统不信任应用程序能够正确地操作外部资源 / 硬件资源。

故操作系统内核直接屏蔽了开发人员对硬件操作的可能，由开发人员向内核发出请求后，再由内核来操作外部资源 / 硬件资源。

CPU 指令集是软件操作硬件的媒介。通过拥有的指令集权限范围来区分用户态和内核态，如内核态拥有完整的指令集，可操作外部资源 / 硬件资源并使用完整的物理内存空间，而用户态只拥有常规的指令集，并且内存空间也被限制在一定范围内。

用户态切换到内核态的**时间开销**主要在于以下几点：

- 保留用户态现场（执行上下文、寄存器、用户栈等）；
- 复制用户态参数，用户栈切换到内核栈，进入内核态；
- 额外的检查（内核对于用户的不信任）；
- 执行内核态代码；
- 复制内核态代码结果，回到用户态；
- 恢复用户态现场（执行上下文、寄存器、用户栈等）。

导致用户态切换到内核态的几种情况：

- 系统调用；
- 异常（内中断），CPU 内部；
- 中断（外中断），CPU 外部。

# 硬件结构

## 存储器金字塔

<img src="pics/存储器分层结构.jpg" alt="存储器分层结构" style="zoom:67%;" />

不同的存储器之间性能差距很大，构造存储器分级很有意义，分级的目的是要构造**缓存**体系。

## 让 CPU 跑得更快的代码

从【数据缓存】以及【指令缓存】两个方面进行探讨：

- 数据缓存：遍历数据时，应根据内存布局的顺序进行操作，这些数据在物理内存上是相连的，再加上 CPU 的预读机制，可以多次命中缓存；
- 指令缓存：有规律的条件分支语句能让 CPU 的【分支预测器】起作用，提前将指令放到指令缓冲中，进一步提高效率。

## CPU 缓存一致性

<img src="pics/CPU-三级缓存.png" alt="CPU-三级缓存" style="zoom: 67%;" />



CPU Cache 的构造：由多个 Cache Line 组成，一个 Cache Line 包含头标志 Tag，数据块 Data Block，Data Block 中可以存放多个数据。

> CPU 和内存交换数据的单位是一个 Cache Line，内存和磁盘交换数据的单位是一个页框（物理内存概念）。

实现一种机制来同步两个不同核心里的缓存数据，为了实现该机制，需要保证以下 2 点：

- **写传播**：即一个核心的更新操作能同步到其它核心；
- **事务的串行化**：即一个核心对缓存的更新操作顺序，在其它核心看起来也必须是一样的。

为了实现写传播，一个常见的方式是**总线嗅探**。

每个核心更新缓存时，就将该事件以**广播**的形式传播到总线上，其它核心不断监听总线上是否有事件发生，如果有，则查看事件所更新数据在自己缓存中是否存在，如果存在，则将自己缓存中的数据更新到和事件中的数据一致。

这种方法的缺点在于，当前核心更新时不管其它核心有没有相同数据都会进行广播，而所有核心需要每时每刻对总线的活动进行监听，最后**该方法无法保证事务的串行化**。

MESI 协议基于总线嗅探机制实现了事务的串行化，并用状态机机制降低了总线带宽压力，做到了 CPU 缓存一致性。

- Modified，已修改

  Data Block 脏标记（CPU 缓存和内存数据不一致），表示 Data Block 已更新，但未写回内存。

  当 Cache Line 要被替换时，需要先将数据写回内存。

- Exclusive，独占

  数据此时是干净的（CPU 缓存和内存数据一致），并且数据只存在于当前缓存中，因此随时进行写入。

  如果其它核心从内存读取了相同数据到缓存，则此时 Cache Line 的【独占】状态将转为【共享】状态。

- Shared，共享

  数据此时是干净的，但是要写入时，**需要向其它核心发送广播**，通知它们将当前 Cache Line 置为【已失效】状态。

  写入后，将自身 Cache Line 置为【已修改】状态，此时可以继续进行修改，并且不需要给其它核心发送消息。

- Invalidated，已失效

  缓存已失效，不可以读取此状态的数据。

MESI 场景举例：

1. A 核心从内存读取变量 i，缓存到自己的 Cache 中，此时其它核心还没有缓存该数据，因此 A 核心标记这个 Cache Line 为【独占】；
2. B 核心从内存读取变量 i，此时发送消息给其它核心，由于 A 和 B 核心都缓存了该数据，因此将各自的 Cache Line 标记为【共享】；
3. A 核心想修改变量 i，发现是【共享】状态，于是发送广播，让其它缓存了相同数据的核心将相应的 Cache Line 标记为【已失效】，然后 A 核心才进行修改，同时将该 Cache Line 标记为【已修改】状态；
4. A 核心继续修改变量 i，由于 Cache Line 为【已修改】状态，此时不需要发送消息，可以直接修改；
5. A 核心要替换变量 i 所在的 Cache Line 时，发现其为【已修改】状态，因此需要先将数据同步到内存。

通过场景举例，可知当 Cache Line 状态为【独占】或【已修改】时，**修改数据**并不需要发送广播，在一定程度上减少了总线带宽压力。

## CPU 伪共享问题

场景分析：

1. A 线程绑定了 1 号核心，B 线程绑定了 2 号核心，A 线程只会读写变量 A，B 线程只会读写变量 B；
2. 1 号核心读取变量 A，由于 CPU 从内存读取数据到 Cache 的单位是 Cache Line，也正好变量 A 和变量 B 的数据归属于同一个 Cache Line，所以 A 和 B 的数据都会被加载 Cache，并将此 Cache Line 标记为【独占】状态；
3. 接着，2 号核心开始从内存里读取变量 B，同样的也是读取 Cache Line 大小的数据到 Cache 中，此 Cache Line 中的数据也包含了变量 A 和变量 B，此时 1 号核心和 2 号核心的 Cache Line 状态变为【共享】状态；
4. 1 号核心需要修改变量 A，发现此 Cache Line 的状态是【共享】状态，需要通过总线发送消息给 2 号核心，通知 2 号核心把 Cache 中对应的 Cache Line 标记为【已失效】状态，然后 1 号核心对应的 Cache Line 状态变成【已修改】状态，并且修改变量 A；
5. 之后，2 号核心需要修改变量 B，此时 2 号核心的 Cache 中对应的 Cache Line 是【已失效】状态，另外由于 1 号核心的 Cache 也有此相同的数据，且状态为【已修改】状态，**所以要先把 1 号核心的 Cache 对应的 Cache Line 写回到内存，然后 2 号核心再从内存读取 Cache Line 大小的数据到 Cache 中**，最后把变量 B 修改到 2 号核心的 Cache 中，并将状态标记为【已修改】状态；

所以，如果 1 号和 2 号 CPU 核心持续交替的分别修改变量 A 和 B，就会重复 4 和 5 两个步骤，**Cache 并没有起到缓存的效果**，虽然变量 A 和 B 之间其实并没有任何关系，但是因为同时归属于一个 Cache Line，这个 Cache Line 中的任意数据被修改后，都会相互影响，从而出现 4 和 5 两个步骤。

因此，这种因为多个线程同时读写同一个 Cache Line 的不同变量时，而导致 CPU Cache 失效的现象称为【伪共享】。

因此，对于多个线程共享的热点数据，即经常会修改的数据，应该避免这些数据刚好在同一个 Cache Line 中，否则就会出现【伪共享】的问题。

在 Linux 中，通过一个宏定义，使得原本可能会放到一个 Cache Line 中的两个变量地址对齐，这样就不会出现在同一个 Cache Line 中了。其实就是通过【空间换时间】的思想，牺牲一部分 Cache 空间，换来性能的提升。

## 中断

<img src="pics/中断.jpg" alt="中断" style="zoom: 80%;" />

<img src="pics/中断2.jpg" alt="中断2" style="zoom:80%;" />



在 Linux 系统中，为了避免由于中断处理程序执行时间过长，而影响正常进程的调度，将中断处理程序分为上半部和下半部：

- 上半部，对应硬中断，由硬件触发中断，用来快速处理中断；
- 下半部，对应软中断，由内核触发中断，【异步】处理上半部未完成的工作。

## 系统调用

参考：[CSDN](https://blog.csdn.net/chosen0ne/article/details/7721550)

系统调用是通过【中断】来实现的，中断有两个重要属性：中断号和中断程序。中断号用来标识不同的中断，不同的中断具有不同的中断处理程序。内核中会维护一个中断向量表，它存储了所有中断处理程序的地址，而中断号就是相应中断在向量表中的偏移量。

在 Linux 下中断号 `0x80` 就是用于进行系统调用的，在 Linux 下共有 435 个系统调用，一个中断号如何处理多个不同的系统调用？最简单的方式就是不同的系统调用采用不同的中断号，但是中断号是一种稀缺资源，Linux 并没有采用这样的做法。

实际上，**Linux 处理系统调用的方式就和中断类似**。每个系统调用都有相应的【系统调用号】作为唯一标识，内核维护一张【系统调用表】，表中元素就是系统调用函数的起始地址，而系统调用号就是系统调用在调用表中的偏移量。只要在进行系统调用（中断）的同时指定系统调用号，就可以明确调用的函数。

Linux 中是通过寄存器 %eax 传递系统调用号，也就是先将系统调用号存入寄存器，然后进行系统调用。

**对于系统调用的参数传递，也是通过寄存器完成的**，Linux 最多允许向系统调用函数传递 6 个参数，分别由 6 个寄存器来进行存储，由于寄存器先前可能被使用，因此需要保存寄存器原来的状态，等待系统调用后再恢复。

在 Linux 中，用户态和内核态进程所使用的栈是不同的，分别是用户栈和内核栈，当进行系统调用时，不仅要切换状态，也要完成栈的切换，这样处于内核态的系统调用才能在内核栈上完成调用。寄存器 %esp （栈指针，指向栈顶）所在的内存空间叫当前栈，比如 %esp 在用户空间则当前栈就是用户栈，否则就是内核栈。栈切换主要就是 %esp 在用户空间和内核空间来回赋值。

# 内存管理

- 虚拟地址空间
- 内存交换
- 分段和分页
- 多级页表
- TLB（局部性原理）

## 为什么需要虚拟内存？

如果没有虚拟内存，每个进程引用的都是绝对的物理内存地址，一旦发生冲突，可能导致进程发生崩溃。

为了避免这种情况，加一个中间层，把每个进程使用的内存地址隔离开来，让操作系统为每个进程分配独立的一套【虚拟地址】，这样每个进程在自己的地址上操作，至于最后怎么落到物理内存里，对进程来说是透明的，由操作系统内核负责这项工作。

## 分段

分段机制下的【虚拟地址】由两部分组成，【段选择子】和【段内偏移量】。

- 段选择子保存在段寄存器里面。段选择子里面最重要的是【段号】，用作【段表】的索引。段表里面保存的是这个段的基地址、段的界限和特权等级等；
- 虚拟地址中的段内偏移量应该位于 0 和段界限之间，如果段内偏移量合法，就将段基地址加上段内偏移量得到物理内存地址。

分段机制的问题：

- 内存碎片：这是因为某个分段内存回收后，没有和空闲内存相邻，于是就出现了“明明有足够空间，但就是无法找到一块合适的大内存加载新进程”的现象。
  - 外部内存碎片：产生了多个不连续的小物理内存，导致新的程序无法找到一块大内存装载。解决的方法就是通过**内存交换**，将运行进程的内存重新加载，紧邻着已有的运行内存，尽量保证空闲内存的连续。
  - 内部内存碎片：程序申请了一部分内存，但是并没有充分利用到申请的内存，这也会导致内存空间的浪费。
- 内存交换效率低：外部内存通过内存交换虽然解决了内存碎片问题，但重新加载是借助 swap 功能将内存数据写到磁盘然后重新加载到内存，如果遇上一大块内存空间进行 swap，就会导致系统的性能下降。

## 分页

将【内存空间】分为一个个大小相等的分区，每个分区就是一个”页框“（页框 = 页帧 = 内存块 = 物理块 = 物理页面）。每个页框有一个编号，即”页框号“（页框号 = 页帧号 = 内存块号 = 物理块号 = 物理页号），页框号从 0 开始。

将【进程的逻辑地址空间】也分为与页框大小相等的一个个分区，每个分区称为一个”页“或”页面“。每个页面也有一个编号，即”页号“，页号也是从 0 开始。

虚拟地址和物理地址之间通过【页表】进行映射。页表存储在内存里，【内存管理单元 MMU】负责将虚拟地址转为物理地址。

当进程访问的虚拟地址在页表中查不到时，会产生【缺页异常】，进入内核态执行相应的处理程序，分配物理内存、更新进程页表，最后返回用户态，恢复进程运行。

分页是如何解决分段中出现的内存碎片和内存交换效率低的问题的？

首先页的单位很小，一般以 KB 为单位，不像段那样动不动就是几百 MB，swap 效率自然也就上去了。

外部内存碎片通过内存交换解决，内部内存碎片因为页本身容量就很小，即使没有充分利用，也不至于出现较大的内存浪费，几乎可以忽略不计。

【虚拟地址】被分为【页号】和【页内偏移量】，通过页号到【页表】找到对应的【物理页号】，然后到物理地址上找到该物理页，再加上偏移量即可得到物理内存地址。

## 多级页表

假设虚拟地址空间有 4GB，一个页的大小是 4KB，总共可以存储 2 ^ 20 张页。为了对这些页进行索引，一个索引项也叫”页表项“至少需要 3B 大小才能定位到这么多张页上，通常取 4B。那么一个页表就需要 4MB 的空间来进行存储。假设系统存在 100 个进程，每个进程需要一个页表，就需要 400MB 的内存空间，非常占用资源。这是简单分页所带来的弊端。

如果把这包含  2 ^ 20 个页表项的页表继续分页，分为 1024 个页表，每个页表包含 1024 个页表项，形成两级分页，其大小为 4B × 1024 = 4KB。

这样一级页表映射 1024 个二级页表，每个二级页表又映射 1024 个页表项，将 2 ^ 20 个页都一一对应起来。

虽然说在 4MB 的基础上又引入了 4KB，但实际上，如果一级页表某个页表项没有被用到，那么也不会创建出对应的二级页表出来，只在需要的时候才进行创建。

一级页表保证覆盖了全部虚拟地址空间，二级页表在需要时被创建。

## 段页式

内存分段和内存分页并不是对立的，完全可以组合起来在一个系统中使用，这被称为段页式内存管理。

- 先将程序划分为多个有逻辑意义的段，也就是前面提到的分段机制；
- 接着把每个段划分为多个页，也就是对分段划分出来的连续空间，再划分固定大小的页。

虚拟地址被分为【段号】、【段内页号】和【页内偏移】三部分组成。数据结构是每个程序有一张段表，每个段又建立一张页表。

Linux 内存主要采用的是页式内存管理，但同时也不可避免地涉及了段机制。这是因为早期 Intel CPU 一律对程序中使用的地址先进行段式映射，然后再进行页式映射。既然 CPU 硬件结构是这样，Linux 内核也只好服从 Intel 的选择。

Intel CPU 内存地址映射流程：【逻辑地址】经过段式内存管理单元得到【线性地址/虚拟地址】，再由页式管理单元得到【物理地址】。

实际上，Linux 内核采取的方法是使段式映射的过程不起作用。

Linux 系统中的每个段都是从 0 开始的整个 4GB 虚拟地址空间（32位），也就是所有段的起始地址都是一样的。这意味着 Linux 系统中的代码，包括操作系统本身的代码和应用程序代码，所面对的地址空间都是线性地址空间（虚拟地址），该做法相当于屏蔽了处理器中逻辑地址的概念，段只被用于访问控制和内存保护。

## 虚拟内存地址映射流程

<img src="pics/虚拟内存管理流程.png" alt="虚拟内存管理流程" style="zoom:67%;" />

## 内存中的堆与栈

[参考文章](https://stackoverflow.com/questions/79923/what-and-where-are-the-stack-and-heap)

栈是为执行线程预留的内存。当一个函数被调用时，会自动在栈的底部开辟一块内存，里面存储了函数执行所需的数据，当函数返回时，该内存自动会被回收。

堆是为动态分配预留的内存。和栈不同，堆中内存没有强制分配或回收的形式，可以在任何时候申请一块内存，并在任何时候去释放它。

每个线程都有一个自己的栈，而应用程序通常只有一个堆。

<img src="pics/堆和栈.jpg" alt="堆和栈" style="zoom:67%;" />

## mmap 系统调用

参考：[博客园文章](https://www.cnblogs.com/huxiao-tee/p/4660352.html)

mmap 是一种内存映射文件的方法，即将一个文件或者其它对象映射到进程的地址空间，**实现文件磁盘地址和进程虚拟地址空间中一段虚拟地址的一一对映关系**。实现这样的映射关系后，进程就可以采用指针的方式读写操作这一段内存，而系统会自动回写脏页面到对应的文件磁盘上，即完成了对文件的操作而不必再调用 read、write 等系统调用函数。相反，**内核空间对这段区域的修改也直接反映用户空间**，从而可以实现不同进程间的文件共享。

mmap 内存映射的实现过程，总的来说可以分为三个阶段：

- 进程启动映射过程，并在虚拟地址空间中为映射创建虚拟映射区域；
- 调用内核空间的系统调用函数 mmap（不同于用户空间函数），实现文件物理地址和进程虚拟地址的一一映射关系；
- 进程发起对这片映射空间的访问，引发缺页异常，实现文件内容到物理内存（主存）的拷贝。

mmap 和常规文件操作的区别：

- 常规文件操作为了提高读写效率和保护磁盘，使用了页缓存机制。这样造成读文件时需要先将文件页从磁盘拷贝到页缓存中，由于页缓存处在内核空间，不能被用户进程直接寻址，所以还需要将页缓存中数据页再次拷贝到内存对应的用户空间中。这样，通过**两次数据拷贝过程**，才能完成进程对文件内容的获取任务。写操作也是一样，待写入的缓冲区在内核空间不能直接访问，必须要先拷贝至内核空间对应的主存，再写回磁盘中（延迟写回），也是需要两次数据拷贝。
- 而使用 mmap 操作文件时，创建新的虚拟内存区域和建立文件磁盘地址和虚拟内存区域映射这两步，没有任何文件拷贝操作。而之后访问数据时发现内存中并无数据而发起的缺页异常过程，可以通过已经建立好的映射关系，**只使用一次数据拷贝，就从磁盘中将数据传入内存的用户空间中**，供进程使用。

mmap 优点总结：

1. 对文件的读取操作跨过了页缓存，减少了数据拷贝次数，用内存读写代替了 I/O 读写，提高文件的读取效率；
2. 实现了用户空间和内核空间的高效交互方式，双方各自的修改操作都可以直接反映在映射的区域内，从而被对方空间及时捕捉；
3. 提供了进程间共享内存以及相互通信的方式，不管是父子进程还是无亲缘关系的进餐，都可以将自身用户空间映射到同一个文件或匿名映射到同一片区域。从而通过各自对映射区域的改动，达到进程间通信和进程间共享的目的；
4. 可用于实现高效的大规模数据传输。内存空间不足，是制约大数据操作的一个方面，解决方案往往是借助硬盘空间协助操作，补充内存的不足。但是进一步会造成大量的文件 I/O 操作，极大影响效率。这个问题可以通过 mmap 映射很好的解决。换句话说，但凡是**需要用磁盘空间代替内存的时候**，mmap 都可以发挥其功效。

# 操作系统结构

## Linux 内核 vs Windows 内核

所谓内核，就是帮助我们应用程序和硬件资源打交道的一个“应用程序”。

内核一般需要提供以下几个能力：

- 进程调度能力；
- 内存管理能力；
- 硬件通信能力；
- 系统调用能力。

内核具有最高权限，而应用程序具有的权限很小，大多数操作系统将内存空间分为内核空间以及用户空间。

用户空间代码只能访问局部的内存空间，而内核空间的代码可以访问所有的内存空间。如果应用程序需要主动进入内核空间，就需要通过【系统调用】。当应用程序使用系统调用时，会产生一个中断。发生中断后，CPU 会中断当前正在执行的用户程序，转而跳转到中断处理程序，即开始执行内核程序。内核处理完后，主动触发中断，把 CPU 执行权交回给用户程序，回到用户态继续工作。

**Linux 内核设计**的理念主要有以下几个点：

- **多任务**；
- **对称多处理**：指每个 CPU 核心的地位都是相等的，不会单独服务某个应用程序或内核程序，每个程序都可以被分配到任意一个核心上执行；
- **可执行文件链接格式 ELF**：它是 Linux 可执行文件的存储格式；
- **宏内核**：意味着 Linux 内核是一个**完整的可执行程序**，且拥有最高的权限。

![内核](pics/内核.jpg)

- 宏内核：特征是内核所有模块都运行在内核态，比如进程调度、内存管理、文件系统、设备驱动等。Linux 的宏内核还实现了动态加载内核模块的功能，例如大部分设备驱动是以可加载模块的形式存在的，与内核其它模块解耦，让驱动开发和加载更方便、灵活；
- 微内核：特征是只保留最基本的能力在内核中，比如进程调度、虚拟内存、中断等，把文件系统、设备驱动放到用户空间，**服务和服务之间是隔离的**（不像宏内核那样是一个完整的程序），单个服务出现故障或被攻击，也不会导致整个操作系统挂掉，提高稳定性和可靠性。微内核的内核功能少，**可移植性高**，相比宏内核的一个缺点在于，由于驱动程序不在内核中，并且驱动程序通常会频繁进行系统调用，于是驱动和硬件交互就需要频繁地切换到核心态，带来性能损耗。华为鸿蒙系统就是微内核结构；
- 混合内核：架构类似微内核，内核里有一个最小版本的内核，然后其它模块在此基础上搭建，实现时跟宏内核类似，把整个内核做成一个完整的程序，类似于以宏内核的方式包裹着一个微内核。

**Windows 内核设计**的理念主要有以下几个点：

- 多任务；
- 对称多处理；
- **可移植执行文件 PE**；
- **混合内核**。

# 进程与线程

## 基础知识

### 进程

运行中的程序，被称为【进程】。

进程有如下基础状态：创建、就绪、运行、阻塞、结束。

处于非运行状态的进程依然占用着物理内存显然是一种浪费内存的行为，在有虚拟地址空间的操作系统中，通常会选择将非运行进程的物理内存空间换出到磁盘，等进程再运行时，再从磁盘换入到物理内存。**使用【挂起】状态来描述进程没有占用实际物理内存空间的情况**。



<img src="pics/进程状态转移.webp" alt="进程状态转移" style="zoom: 50%;" />



导致进程挂起除了操作系统自身的原因，还有：

1. 通过 `sleep` 函数让进程短暂挂起，原理是通过设置一个定时器，到期后唤醒进程；
2. 用户希望挂起程序的执行，比如在 Linux 使用 Ctrl + Z 挂起进程。

**使用【进程控制块】来描述进程信息，PCB 是进程存在的唯一标识。**

PCB 包含以下信息：

- 进程描述信息：进程标识符和用户标识符；
- 进程控制和管理信息：进程状态、优先级；
- 资源分配清单：**内存**地址空间、所打开的**文件**列表、所使用的 **I/O 设备**信息；
- CPU 相关信息：**CPU 各个寄存器的值**，以便在进程重新执行时，能从断点处继续执行。

> CPU 上下文切换就是先把前一个任务的 CPU 上下文（CPU 寄存器和程序计数器）保存到内核，然后加载下一个任务的上下文到这些寄存器和程序计数器，最后再跳转到程序计数器所指的新位置，运行下一个任务。在这里【任务】可以被替换成进程、线程、中断。

**进程由内核统一管理和调度，因此进程的切换只能发生在内核态。**

**进程切换的上下文不仅包括虚拟内存、栈、全局变量等用户空间资源，还包括内核堆栈、寄存器等内核空间资源。**

**进程上下文切换所需的时间开销**包括：

1. 用户态和内核态之间的转换；
2. 保存上一个进程的上下文和加载下一个进程的上下文。

通常说的，尽可能减少进程上下文切换所带来的开销，其实就是减少进程上下文切换的**次数**从而减少 CPU 所浪费的时间。

进程上下文切换的常见场景：

1. 时间片耗尽；
2. 系统资源不足；
3. sleep 函数；
4. 优先级；
5. 硬件中断。

进程空间分布：

1. **程序段**：存放方法体的二进制代码；
2. **初始化数据**：程序运行初期已经初始化的数据；
3. **未初始化数据**：程序运行初期未初始化的数据；
4. **栈**：存储局部、临时变量，函数调用时，存储函数的返回指针，用于控制函数的调用和返回。在程序块开始时自动分配内存，结束时自动释放内存；
5. **堆**：存储动态内存分配，需要程序员手动分配、手动释放。

### 线程

> 需要多线程的主要原因是，在许多应用中同时发生着多种活动。其中某些活动随着时间的推移会被阻塞。通过将这些应用程序分解成可以并行运行的多个顺序线程，程序设计模型会变得简单。
>
> 线程的本质就是可与其它线程共享同一个内存空间的进程。

**线程是进程当中的一条执行流程。**多个线程之间可以共享代码段、数据段、打开的文件等资源，但每个线程各自都有一套独立的寄存器和栈，可以**确保线程的控制流是相对独立的**。

线程优点：

- 一个进程中可以同时存在多个线程；
- 各个线程之间可以并发执行；
- 各个线程之间可以共享进程地址空间和文件等资源。

线程上下文切换的内容依据是否为同一个进程内部线程来区分，如果非同一个进程，则上下文切换和进程上下文切换一致，如果同一个进程，上下文切换仅需切换线程的私有数据、寄存器等不共享的数据。

### 进程和线程区别

- 进程是资源分配的单位，线程是 CPU 执行调度的单位；
- 进程拥有完整的资源平台，而线程只独享必要的资源，比如程序计数器和栈；
- 双方都有就绪、阻塞、执行这三种基本状态；
- 线程能减少并发执行时的时间和空间开销。

对于线程比起进程能够减少时间和空间开销，体现在：

- 线程所需资源少，创建和销毁线程耗费时间少；
- 线程切换不涉及页表切换（共享虚拟内存），进程切换涉及页表切换，页表切换过程开销较大；
- 线程之间通过共享内存的方式传递数据，不需要经过内核，效率更高。

### 纤程的概念

> 纤程就是可以与其它纤程同时在一个线程内运行的线程。

Java 线程是 1:1 映射的内核线程，线程切换需要借助操作系统，从用户态转为核心态，过程中需要进行响应中断、保护和恢复执行现场。

虽然线程切换的开销比起进程切换来说已经相对较小，但是线程的数量一旦过多，开销也是不容小觑，另一方面，过多的线程也会占用大量的内存（数量在百级别千级别）。

为什么需要那么多的线程？因为如今互联网的环境中，一个大型业务系统面对的请求数量级别都是在百万以上。

既然线程之间切换会产生开销，那不进行线程切换，我们在**单线程**内部自己模拟”多线程“，由程序员自己在用户线程中进行”多线程“编程，自己维护上下文现场。

这种线程内部的”多线程“就被称为纤程，它发生在单个用户线程内部，需要程序员自己进行保护、恢复现场及调度的工作。

在调度工作的基础上，又要进行保护、恢复现场的工作，对于程序员编程来说是一项更高的考验。

目前，Java 实现纤程，比较流行的做法是借助 Quasar 框架，它在编译期间，将局部变量以字节码的形式编入到字节码文件中从而实现上下文的保存和恢复，不需要程序员来进行这些工作。

## 进程的创建原语

1. 分配一个唯一的进程 ID，并申请一个空白的 PCB，若 PCB 申请失败则进程创建失败；
2. 为进程分配内存资源，当资源不足时，不会创建失败，而是处于阻塞状态，等待内存资源；
3. 初始化 PCB；
4. 如果进程就绪队列能够接收新进程，则放入就绪队列中。

## 进程的内存布局

由高位地址到低位地址，进程的内存布局分别是：

1. 内核空间；
2. 栈（存放局部数据）；
3. 堆（存放动态分配数据）；
4. 数据段1（未初始化的全局数据和静态数据）；
5. 数据段2（全局数据和静态数据）；
6. 代码段。

## 进程中线程的可创建数量

![内存空间分配](pics/内存空间分配.jpg)

- 32 位系统，用户态的虚拟空间只有 3G，如果创建线程时分配的栈空间是 10M，那么一个进程最多只能创建 300 个左右的线程。
- 64 位系统，用户态的虚拟空间大到有 128T，理论上不会受虚拟内存大小的限制，而会受系统的参数或性能限制。

## fork 系统调用

父进程在通过 `fork` 系统调用生成子进程时，操作系统会把父进程的【页表】复制一份给子进程，这个页表记录着虚拟地址和物理地址的映射关系，而不会复制物理内存，也就是说，两者的虚拟空间不同，但其对应的物理空间是同一个。

<img src="pics/fork-1.jpg" alt="fork-1" style="zoom:50%;" />

这样一来，子进程就共享了父进程的物理内存数据了，这样能够**节约物理内存资源**，页表对应的页表项的属性会标记该物理内存的权限为**只读**。

不过，当父进程或者子进程向该物理内存发起写操作时，CPU 就会触发**缺页中断**，这个缺页中断是由于**违反权限**导致的，然后操作系统会在【缺页异常处理函数】里进行**物理内存的复制**，并重新设置其内存映射关系，将父子进程的内存读写权限设置为**可读写**，最后才会对内存进行写操作，这个过程被称为【写时复制】。

<img src="pics/fork-2.jpg" alt="fork-2" style="zoom:50%;" />

写时复制顾名思义，**在发生写操作时，操作系统才会去复制物理内存**，这样是为了防止 `fork` 创建子进程时，由于物理内存数据的复制时间过长而导致父进程长时间阻塞的问题。

当然，操作系统复制父进程页表的时候，父进程也是阻塞中的，不过页表的大小相比实际的物理内存小很多，所以通常复制页表的过程是比较快的。

不过，如果父进程的内存数据非常大，那自然页表也会很大，这时父进程在通过 `fork` 创建子进程时，阻塞时间也就越久。

未完待续。。。

## 孤儿进程和僵尸进程

### 孤儿进程

父进程退出，它的一个或多个子进程仍在运行，这些子进程将成为孤儿进程。

孤儿进程会被 init 进程（进程号为 1）所收养，并由 init 进程对它们完成状态收集工作。

由于孤儿进程会被 init 进程收养，所以孤儿进程不会对系统造成危害。

### 僵尸进程

一个子进程的进程描述符在子进程退出时不会释放，只有当父进程通过 `wait()` 或 `waitpid()` 获取了子进程信息后才会释放。如果子进程退出，而父进程并没有调用 `wait()` 或 `waitpid()`，那么子进程的进程描述符仍然保存在系统中，这种进程称之为僵尸进程。

僵尸进程通过 `ps` 命令显示出来的状态为 Z（zombie）。

系统所能使用的进程号是有限的，如果产生大量僵尸进程，**将因为没有可用的进程号而导致系统不能产生新的进程**。

要消灭系统中大量的僵尸进程，只需要将其父进程杀死，此时僵尸进程就会变成孤儿进程，从而被 init 进程所收养，这样 init 进程就会释放所有的僵尸进程所占有的资源，从而结束僵尸进程。

## 进程间通信

- 管道

  管道分为【匿名管道】和【命名管道】。

  Linux 命令行中常见的 `|` 竖线就是一个匿名管道，使用结束后就销毁。命名管道则需要通过命令 `mkfifo` 来创建，以文件的形式一直存在。

  管道这种通信方式效率低，**不适合进程频繁地交换数据**。好处是简单，并且容易得知管道内的数据已经被另一个进程读取了。

  不管是匿名管道还是命名管道，读取和写入数据都是在内核中进行，之间通信数据遵循**先进先出**原则，不支持 `lseek` 之类的文件定位操作。

  匿名管道的原理是通过 fork 子进程的方式实现的，匿名管道创建时会返回一个写端描述符和读端描述符，父子进程之间一个关闭读端描述符，一个关闭写端描述符，这样就形成了一个进程读管道和一个进程写管道的相互通信功能（fork 会复制描述符，因此需要关闭多余的描述符）。对于命令行也就是 shell 进程来说，使用命令 `A | B` 进行管道通信时，A 和 B 就相当于 shell 的两个子进程。

- 消息队列

  消息队列虽然**可以让进程之间频繁地交换数据**，但是它不适合大数据传输（消息体和队列本身都有最大长度限制），另外，消息队列是一个存在于**内核**中的消息链表（如果没有释放或者关闭操作系统，消息队列会一直存在），因此进程写入和读取需要进行上下文切换，会产生**数据拷贝的开销**。

- 共享内存

  共享内存不会产生数据拷贝的开销，它通过让两个进程拿出一部分虚拟地址空间出来，映射到同一块**物理内存**上，避免通信数据的频繁移动。

  共享内存只有在两个进程都使用了类似文件关闭 `close` 的系统调用 `shmdt` 后，共享内存才会被删除，或者直到系统被重启。

- 信号量

  信号量是一种整型计数器，用于进程和线程之间的互斥和同步操作（这两种操作本身就属于通信的一种方式），它不用于缓存通信数据。

  通常信号量初始化为 `0` 表示是一个同步信号量，初始化为 `1` 表示是一个互斥信号量。另外，控制信号量有两种原子操作，一种是 `P` 操作，一种是 `V` 操作。

- 信号

  上述都是常规状态下的进程通信方式，而**在异常状态下，就需要使用【信号】来通知进程**。

  **信号和信号量之间没有任何关系，两者用途也完全不同。**

  Linux 通过命令 `kill -l` 可以查看所有的信号。

  那些运行在终端的进程，可以通过组合键来向其发送信号，比如 CTRL + C 发送 `SIGINT` 信号表示终止该进程，CTRL + Z 发送 `SIGTSTP` 信号来挂起进程。

  那些运行在后台的进程，可以通过 `kill` 命令指定信号类型以及 PID 向指定进程发送信号，`kill` 命令缺省信号是 `SIGTERM` 信号。

  信号是进程间通信**唯一的异步通信机制**，可以在任意时候向进程发送信号。

  用户进程对于信号处理，也有如下几种方式：

  1. 执行默认操作，即执行 Linux 为信号所规定的默认操作；
  2. 捕捉信号，为信号定义一个处理函数，当信号到达时，不执行默认操作，而是执行信号处理函数；
  3. 忽略信号，如字面意思，但有两个信号，用户进程不能捕捉也不能忽略（这表示必须执行默认操作），即 `SIGKILL` 和 `SIGSTOP`。

- Socket

  Socket 通信不仅可以跨网络与不同主机的进程间通信，同主机上的进程也可以使用 Socket 进行通信。

  本地 Socket 通信只需要**绑定一个本地文件**即可，不需要绑定 IP 地址和端口。

## 多线程同步

互斥是什么意思？

说到互斥，需要先说到临界区，临界区指的是包含共享资源的一个代码段，在多线程场景下，如果未对临界区施加保护，可能会造成意想不到的结果。

**互斥，即保证只有一个线程在临界区内运行，此时其它线程应该在临界区外进行等待。**

同步是什么意思？

**并发的进程或者线程在一些关键点上可能需要互相等待与互通信息，这种相互制约的等待与互通信息称为进程/线程同步。**

互斥与同步的实现方式：

- 锁，可以实现互斥；
- 信号量，可以实现互斥与同步。

### 生产者-消费者



### 哲学家就餐

描述：五个哲学家，围着一张圆桌吃面，巧在桌上只有五支叉子，每两个哲学家之间放一支叉子，哲学家会进行思考，思考饿了就开始进餐，问题是，哲学家必须拿到左右两支叉子组成一对后才会进餐，吃完后会将叉子返回原处，继续思考。

要求保证每个哲学家能够顺利进餐，并且不会出现永远有人拿不到叉子的情况。

<img src="pics/哲学家1.jpg" alt="哲学家1" style="zoom:50%;" />

该方案的缺点在于，如果每个哲学家同时拿了自己左边的叉子，就会导致**死锁现象**，也就是每个哲学家都会在第二个 `P` 操作处阻塞。

<img src="pics/哲学家2.jpg" alt="哲学家2" style="zoom:50%;" />

既然前面方案会发生【同时竞争左边叉子】导致死锁的现象，那就在拿叉子前，加一个【互斥】信号量，只要有一个哲学家进入了临界区，也就是准备拿叉子时，其他哲学家都不能动，只有等到当前哲学家进餐结束，离开临界区后，才能轮到下一个哲学家进餐。

该方案的缺点在于，虽然不会发生死锁现象，但**每次只有一个哲学家在进餐**，明明五支叉子可以供最多两个哲学家同时进餐，从效率上讲，不是一个好的方案。

<img src="pics/哲学家3.jpg" alt="哲学家3" style="zoom:50%;" />

既然互斥信号量会导致只有一个哲学家在进餐，那就取消互斥信号量，再结合方案一是由于同时拿了左边的叉子导致了死锁，所以可以考虑从避免同时拿左边叉子的方面，即根据哲学家编号的不同，采取不同的动作。

该方案不会出现死锁，也可以同时两人进餐。

### 读者-写者

- 读者优先

  <img src="pics/读写锁1.jpg" alt="读写锁1" style="zoom:50%;" />

  1. 写写之间互斥，因此需要一个互斥信号量；
  2. 读读之间可以并发，因此准备一个读者计数器对当前正在读的线程进行计数；
  3. 计数器属于共享资源，需要一个互斥信号量保证正确修改；
  4. 因为是读者优先，当第一个读者开始读时，便阻塞后到的写者，直到最后一个读者离开时，才唤醒被阻塞的写者。

- 写者优先

  <img src="pics/读写锁2.jpg" alt="读写锁2" style="zoom:50%;" />

  1. `rMutex` 的作用在于，开始有多个读者读数据，则它们全部进入读者队列，此时来了一个写者，执行了 `P(rMutex)` 之后，后续的读者由于阻塞在 `rMutex` 上，都不能再进入读者队列，而写者到来，则可以全部进入写者队列，因此保证了写者优先；
  2. 同时，第一个写者执行了 `P(rMutex)` 之后，也不能马上开始写，必须等到所有进入读者队列的读者都执行完读操作，通过 `V(wDataMutex)` 唤醒写者的写操作。

- 公平竞争

  <img src="pics/读写锁3.jpg" alt="读写锁3" style="zoom: 50%;" />

  1. 在读者优先的基础上，增加一个公平信号量 `flag`；
  2. 对比方案一可以发现，读者优先只要后续有读者到达，读者就可以进入读者队列，而写者必须等待，直到没有读者到达；
  3. 没有读者到达会导致读者队列逐渐为空，即 `rCount == 0`，此时写者才可以进入临界区执行写操作；
  4. `flag` 的作用就在于阻止读者的这种只要读者到达，就可以进入读者队列的特殊权限；
  5. 假设此时来了一个写者，执行 `P(flag)` 操作，使得后续到达的读者都阻塞在 `flag` 上，不能进入读者队列，会**使得读者队列逐渐为空**。

## 死锁

死锁简单来说就是两个线程各自持有对方请求的资源而互相等待的过程。

死锁只有**同时满足**以下四个必要条件时才会发生：

- 互斥条件：多个线程不能同时持有同一个资源；
- 持有并等待条件：线程 A 在等待资源 2 的同时不会释放持有的资源 1；
- 不可剥夺条件：已持有的资源在未使用完之前不能被其它线程获取；
- 环路等待条件：两个线程获取资源的顺序构成了环路。

处理死锁有四种方法：

- 鸵鸟策略

  忽略死锁，因为处理死锁的代价很高，当死锁发生不会对用户造成较大影响或死锁发生的概率很低，可以使用该方法。

- 死锁检测和死锁恢复

  死锁检测分为【每种类型一个资源的死锁检测】和【每种类型多个资源的死锁检测】。

  死锁恢复分为【利用抢占恢复】、【利用回滚恢复】、【通过杀死进程恢复】。

- 死锁预防

  **在程序运行之前预防死锁发生。**

  要**预防**死锁问题，只需破坏其中一个必要条件即可。**通常使用资源有序分配法，来破坏环路等待条件**。

- 死锁避免

  **在程序运行时避免发生死锁，从本质上来讲死锁避免是不可能的，因为它需要获知未来的请求。**

  1. 安全状态

     说明当前没有死锁发生，并且即使所有进程忽然请求对资源的最大需求，也**存在某种调度次序**能够使得每一个进程运行完毕，则称此状态是安全的。

  2. 单个资源的银行家算法

     对每一个请求进行检查，检查如果满足该请求后是否能进入**安全状态**，如果能，就满足该请求，否则推迟该请求的满足。

     ![银行家-单个](pics/银行家-单个.png)

  3. 多个资源的银行家算法

     从表中找到一个进程的资源向量小于等于剩余资源向量，如果找不到，则系统将发生死锁，该状态是不安全的。

     如果找到这样的一个进程，则将资源分配给该进程，满足其运行，随后回收进程资源到剩余资源中。

     重复上述步骤，直到所有进程都得到运行，则该状态是安全的。

     ![银行家-多个](pics/银行家-多个.png)

## 操作系统中的锁

### 互斥锁与自旋锁

**它俩是最底层的锁**。其它高级的锁都是基于它俩进行实现的。

它俩区别在于加锁失败后的处理方式：

- 互斥锁加锁失败后，立即释放 CPU，进入等待队列等待；
- 自旋锁加锁失败后，忙等待，直到拿到锁。

互斥锁加锁失败而阻塞的现象，由操作系统内核实现。

所以，互斥锁加锁失败后，会从用户态陷入到内核态，由内核完成线程状态的切换，简化使用锁的难度，但是存在性能开销成本。

开销成本是两次线程上下文切换的成本：

- 加锁失败，内核将线程状态从【运行】切换为【阻塞】；
- 锁被释放，内核将线程状态从【阻塞】切换为【就绪】。

上下文切换的耗时大致在十几纳秒到几微秒之间，**如果临界区代码执行时长较短，可能会出现上下文切换时间比代码执行时间还要长**。

**如果能确定临界区代码执行时长较短，则应该使用自旋锁。**

自旋锁通过 CPU 提供的 CAS 函数，在【用户态】完成加锁和解锁操作，**不会主动产生线程上下文切换**，比起互斥锁来说，更快，开销也更小。

自旋锁加锁包含两步骤：1. 查看锁状态，锁空闲，执行下一步，2. 将锁设置为当前线程持有，两步骤会被合并为一条硬件级指令，即原子操作。

**需要注意的是，在单核 CPU 上需要借助抢占式的调度器（通过时钟中断一个线程，运行其它线程），否则，自旋锁在单核 CPU 上无法使用，会一直占用单核的 CPU 周期。**

需要清楚地知道，**自旋时间和代码执行时间是呈正比的**。

### 读写锁

分为读锁和写锁，**适用于能明确区分【读操作】和【写操作】的场景**。

最基本的读写锁也被称为【读优先锁】，适合【读多写少】的场景。

相对应的，存在【写优先锁】，即一个写锁尝试获取锁时，后续的读锁会加锁失败。

【公平读写锁】比较简单的一个思路是借助【队列】对获取锁的线程进行排队，按照先进先出的原则加锁即可，仍可并发，也不会出现【饥饿】现象。

**读写锁可以根据场景选择互斥锁或自旋锁其中一个进行实现。**

### 乐观锁与悲观锁

上述提到的互斥锁、自旋锁、读写锁都属于【悲观锁】，它们都需要【加锁】和【解锁】操作。

【悲观锁】认为多线程同时修改共享资源的概率比较高，容易出现冲突，所以访问资源前需要加锁。

【乐观锁】则认为同时修改共享资源的概率比较低，不采用【加锁】和【解锁】操作，也叫【无锁编程】。

乐观锁的工作方式是：**先修改共享资源，如果这段时间内没有其它线程修改过资源，则修改成功，如果有，则放弃本次操作**。

至于放弃操作后需不需要重试，这和业务场景有关。即使重试成本高，但冲突概率低的话，还是可以接受重试成本的。

# 调度算法

## 进程调度算法

- 先来先服务
  - 对长作业有利，适用于 CPU 密集型场景，不适用于 I/O 密集型场景。
- 最短作业优先
  - 对短作业有利，会造成长作业的”饥饿“现象。
- 高响应比优先
  - 优先权 = ( 等待时间 + 要求服务时间 ) / 要求服务时间；
  - 等待时间相同时，要求服务时间越短，响应比越高，对短作业有利；
  - 要求服务时间相同时，等待时间越长，响应比越高，**兼顾**了长作业。
- 时间片轮转
  - 时间片太短会导致进程上下文切换频繁，降低 CPU 效率；
  - 时间片太长会导致短作业的响应时间变长。
- 最高优先级
  - 静态优先级和动态优先级；
  - 非抢占式和抢占式。
- 多级反馈队列
  - 设置多个队列，优先级从高到低，时间片从小到大；
  - 新进程会被放入第一级队列末尾，如果在当前队列规定的时间片没有运行完成，则放入下一级队列末尾，以此类推；
  - 如果进程运行时，有新进程进入较高优先级队列，则停止当前进程放入原队列末尾，运行较高优先级进程。

## 内存页面置换算法

### 缺页中断过程

<img src="pics/缺页中断.webp" alt="缺页中断" style="zoom:67%;" />

### 页面置换算法

针对缺页中断过程中的第 4 步：页面换入到空闲的物理页，**如果不存在空闲的物理页时，就需要选择一个物理页面对其进行换出**。

**缺页中断的次数和页面置换的次数不一定相等，页面置换的前提是无空闲物理页可被装入，缺页中断则是在页表访问不到指定页时发生**。

页面置换算法的目标是**尽可能减少页面的换入和换出次数（对于常用页面，应尽可能长时间保存在物理内存中）**。

- 最佳页面置换算法 OPT

  **置换在未来最长时间不访问的页面**。

  该算法本质上是无法实现的，因为需要知道未来的请求，因此该算法常被用于衡量其它算法的效率。

- 先进先出置换算法 FIFO

  **置换在内存驻留时间最久的页面**。

- 最近最久未使用置换算法 LRU

  **选择最长时间没有被访问的页面进行置换**。因此，一个页面如果被一直访问，即使它驻留了非常久的时间，也不会被置换出去。

  LRU 理论上可以实现，但代价很高。它需要在内存中维护一张**所有页面**的链表，最近最多使用的页面在表头，最近最少使用的页面在表尾。

  并且**每次访存**时都需要更新链表，找到指定页面节点，删除，然后重新放到表头，链表的查询导致这是一项非常耗时的操作。

- 时钟页面置换算法 LOCK

  它和 LRU 近似，又是对 FIFO 的一种改进。它将所有的页面组织成一个【环形链表】，一个表针指向最老的页面，当发生页面置换时：

  1. 检查当前指针所指向的页面，如果访问位为 0，则淘汰该页面，将新页面放入该位置，指针前移一个位置；
  2. 如果当前访问位为 1，则清除访问位，指针前移一个位置，重复该过程直到找到访问位为 0 的页面。

- 最不常用置换算法 LFU

  **选择访问次数最少的页面进行置换**。

  实现方式就是为每一个页面设立一个【访问计数器】，当需要页面置换时，将最少次数的页面置换出去。

  但是，为每一个页面设立计数器，以及遍历所有页面找到计数最少的页面，这两个工作的效率和硬件成本是需要被考虑的。

  另外，**LFU 只考虑频率问题，没有考虑时间问题**。

  比如，前段时间有个页面被频繁访问，现在变成另一个页面被频繁访问，但是另一个页面没有先前页面的访问次数多，这就导致另一个正在被频繁访问的页面在需要页面置换时被置换出去。

  一个可行的解决办法是，**定期减少访问的次数**，比如当发生时间中断时，把过去时间访问的页面访问次数除以 2，随着时间的流逝，之前高访问的页面访问次数会逐渐减少，增大了置换的概率。

## 磁盘请求调度算法

磁盘请求调度算法就是如何**组织一个磁盘请求序列的访问顺序**，尽可能地减少磁盘 I/O 所耗费的时间。

- 先来先服务

  如果有大量进程竞争使用磁盘，可能会出现请求的磁道较**分散**的情况，此时先来先服务的寻道时间就会变得过长。

- 最短寻道时间优先

  如果请求序列源源不断地有新请求加入，这些请求刚好集中于当前磁头的附近，这就导致一些距离较远的磁道请求长时间无法得到响应，出现了**饥饿**现象。

- 扫描

  最短寻道时间优先产生饥饿的原因在于：磁头有可能在一小块区域内来回移动。

  扫描算法也叫电梯算法，指的是当前磁头先朝一个方向逐个处理请求，并在到达最小或最大磁道号后才反向处理其它请求。

  不会产生饥饿，但是中间部分的磁道比其它部分的磁道响应频率更高，即**每个磁道的响应频率存在差异**。

- 循环扫描

  朝一个方向扫描到达最小或最大磁道号后，直接回到另一端的开始，再进行扫描，即使中间有其它请求存在。

  相当于如果起始是按照磁道号从小到大响应请求，那么后续也是按照从小到大响应请求。

  比起扫描算法，**磁道的响应频率比较平均**。

- LOOK 和 C-LOOK

  扫描和循环扫描都是需要到达最小或最大磁道号后才进入下一轮的扫描。

  LOOK 和 C-LOOK 就是分别针对扫描和循环扫描的优化，优化手段是到达最远距离的请求后就进入下一轮扫描，不需要到达最小或最大磁道号。

# 磁盘

## 磁盘结构

磁盘的物理地址：
- 柱面号：其实就是磁道号，磁盘的盘面被分为多个磁道，于是多个盘面对应位置的磁道组成了一个柱面；
- 盘面号：一个硬盘包含多个盘面，通过激活对应盘面上的磁头来读取数据；
- 扇区号：每个磁道被分为多个扇区，各个扇区存放的数据量相同，因此最内侧磁道的扇区面积小，数据密度最大。

[知乎文章](https://zhuanlan.zhihu.com/p/117375905)

[华为云文章](https://www.huaweicloud.com/articles/d5ccb6e88ce440c85bd48e5faceabd91.html)

读取磁盘数据最终是定位到了某个扇区并开始读取，但实际上操作系统并不直接与扇区交互，而是与多个【连续】扇区组成的【磁盘块】交互。在 Linux 中，可以通过 `sudo /sbin/fdisk -l` 命令来查看扇区大小，默认是 512B。

操作系统通过抽象出磁盘块的逻辑概念，和磁盘块打交道，因此磁盘最小单位是扇区，操作系统通过文件系统和磁盘打交道，文件系统最小单位是磁盘块。在 Linux 中，可以通过 `sudo stat /boot` 命令来查看磁盘块大小，默认是 4KB。

内存操作的最小单位是页框，在 Linux 中，可以通过 `sudo getconf PAGE_SIZE` 命令来查看页框大小，默认是 4KB。

通常情况下，页框大小为磁盘块大小的 2^n 倍，磁盘块大小为扇区大小的 2^n 倍。

## 磁盘调度算法

一次磁盘读/写操作需要的时间：
- **寻道时间**：启动磁臂，**移动磁头所需要的时间**（磁盘调度算法的指标）；
- 延迟时间：将目标扇区旋转到磁头下面的时间；
- 传输时间：读/写数据花费的时间。

# 文件系统

## 基本组成

Linux 文件系统会为每个文件分配两个数据结构：索引节点和目录项，它们主要用来记录文件的元信息和目录层次结构。

- 索引节点，也就是 inode，用来记录文件的元信息，比如 inode 编号、文件大小、访问权限、创建时间、修改时间、**数据在磁盘的位置**等等。索引节点是文件的**唯一**标识，它们之间一一对应，都被存储在磁盘中，所以**索引节点同样占用磁盘空间**；
- 目录项，也就是 dentry，用来记录文件的名字、**索引节点指针**以及与其它目录项的层级关系。多个目录项关联起来，就会形成目录结构，但它与索引节点不同的是，**目录项是由内核维护的一个数据结构，不存放于磁盘，而是缓存在内存**。

## 文件 I/O

- 缓冲 I/O 与非缓冲 I/O

  根据【是否利用**标准库**缓冲】，可以将文件 I/O 分为缓冲与非缓冲 I/O。

  这里的缓冲特指标准库内部实现的缓冲，例如很多程序遇到换行时才真正输出，而换行前的内容都被标准库暂时缓存起来，这样做的目的是减少系统调用的次数，减少上下文切换的开销。

- 直接 I/O 与非直接 I/O

  根据【是否利用**操作系统缓存**】，可以将文件 I/O 分为直接与非直接 I/O。

  即用户读写磁盘时，会经过内核中的磁盘高速缓存，这就是非直接 I/O，而直接越过磁盘高速缓存进行读写，这就是直接 I/O。

- 阻塞与非阻塞 I/O VS 同步与异步 I/O

## 为什么 SSD 不能作为内存使用

- 速度仍有差异，即使速度再快，也和内存差了一个数量级；
- SSD 是以块的粒度组织数据的，而对内存则是能够以字节级别的粒度进行寻址（每个字节都有它的内存地址），CPU 可以直接通过这个地址获取到相应的内容，**也就是说 CPU 没有办法直接访问 SSD 文件中某个特定的字节**；
- CPU 访问 SSD 文件内容需要借助文件系统将内容放到对应进程的内存中，然后操作内存进行访问；
- 实际上 SSD 被操作系统**间接**地作为内存来使用，比如通过 swap 技术将一些阻塞进程的内存数据放到磁盘中的 swap 分区，这部分内存就可以空出来给其它进程使用，内存被 swap 到磁盘分区的进程此时状态为挂起状态。

## 软硬链接

软硬链接是一类特殊的文件，它们的作用在于为某个文件**取别名**。

硬链接是多个目录项中的【索引节点】指向一个文件，即指向同一个 inode，硬链接不可用于跨文件系统。由于多个目录项都指向同一个 inode，只有删除文件的所有硬链接和源文件时，系统才会彻底删除该文件。

软链接相当于**重新创建一个文件**，该文件有独立的 inode，但是这个文件的内容是另一个文件的路径，所以访问软链接的时候相当于访问了另一个文件，软链接是可以跨系统的，甚至目标文件删除后，链接文件还是在的，只不过指向的文件找不到了。

# 设备管理

## 键盘敲击到屏幕显示

<img src="pics/总线结构.webp" alt="总线结构" style="zoom:67%;" />

1. 【键盘控制器】产生扫描码数据，缓存在键盘控制器的寄存器中，通过总线向 CPU 发送**中断请求**；
2. CPU 收到中断请求，**保存被中断进程的 CPU 上下文**，调用键盘设备的**中断处理程序**，该程序在**键盘驱动程序**初始化时被注册到系统中；
3. 键盘的中断处理程序功能就是从键盘控制器的寄存器读取扫描码，转换扫描码到输入字符再到显示字符最后到 ASCII 码，将 ASCII 码放到【读缓冲区队列】；
4. **显示器驱动程序**会定时从【读缓冲区队列】读取数据到【写缓冲区队列】，最后把【写缓冲队列】的数据一个一个写入到【显示控制器】的寄存器中，最终将这些数据显示在屏幕上；
5. 显示结果后，**恢复被中断进程的上下文**。

# 网络系统

## 零拷贝

### 为什么要有 DMA 技术

<img src="pics/传统-IO.webp" alt="传统-IO" style="zoom:67%;" />



在没有 DMA 技术之前，用户进程发起一次文件读入请求，CPU 需要：

1. 向磁盘发起 I/O 请求，然后返回；（CPU 可以忙活其它进程，当前用户进程则处于阻塞等待）
2. 磁盘将数据放入磁盘控制器缓冲区，向 CPU 发送中断信号；
3. CPU 收到中断信号后，将数据从磁盘控制器缓冲区拷贝到内存的 PageCache 中，再从 PageCache 拷贝到内存的用户缓冲区。

可以看到 CPU 需要负责【两次】数据拷贝操作，期间不能进行其它操作。



<img src="pics/DMA-IO.png" alt="DMA-IO" style="zoom:67%;" />



引入 DMA 技术后，用户进程发起文件读入请求，CPU 和 DMA 需要：

1. CPU 向 DMA 发送 I/O 请求，然后返回；
2. DMA 向磁盘发送 I/O 请求，然后返回；
3. 磁盘将数据放入磁盘控制器缓冲区，向 DMA 控制器发出中断信号；
4. DMA 收到中断信号后，从磁盘控制器缓冲区拷贝数据至内存的内核缓冲区中，向 CPU 发送中断信号；
5. CPU 收到中断信号，将数据从内核的内核缓冲区拷贝到内存的用户缓冲区中。

可以看到 CPU 只需负责【一次】数据拷贝操作，并且是将内存的一部分拷贝到另一部分，不再参与从磁盘到内存的数据搬运工作。

### 传统文件传输

假设服务端需要向客户端提供文件传输功能，最简单的方式就是：从磁盘读取文件，然后通过网络协议发送给客户端。

```c++
read(file, tmp_buf, len);
write(socket, tmp_buf, len);
```



<img src="pics/传统文件传输.webp" alt="传统文件传输" style="zoom:67%;" />



根据上图分析，可知一共发生了 4 次用户态和内核态的切换，以及 4 次数据拷贝操作。

要想提高文件传输的性能，就需要从减少【上下文切换次数】和【数据拷贝次数】入手。

### 如何优化文件传输的性能

- 减少上下文切换次数

  之所以会发生上下文切换，这是因为处于用户态的进程并没有权限去操作磁盘和网卡，必须通过内核来进行操作。

  调用 1 次系统函数，就会发生 2 次上下文切换：用户态到内核态，内核态到用户态。

  **所以，要减少上下文切换次数，就需要减少系统调用的次数。**

- 减少数据拷贝次数

  在文件传输场景中，用户空间通常不会对数据进行再加工，因此不需要搬运到用户空间，**即不需要用户缓冲区**。

### 如何实现零拷贝

#### mmap + write

通过上述分析可知在传输文件场景中可以不需要用户缓冲区，因此可以使用 `mmap` 来代替 `read` 函数。



<img src="pics/mmap-传输.webp" alt="mmap-传输" style="zoom:67%;" />



`mmap` 函数的作用在于它会**将内核态缓冲区里的数据映射到用户空间**，这样进程在用户空间进行操作时就可以间接操作内核缓冲区。

通过该方案，数据拷贝次数从原来的 4 次变成了 3 次，但仍需要 4 次上下文切换。（这种方式并不是最理想的零拷贝

#### sendfile

在 Linux 2.1 版本中，提供了一个**专门**用于文件传输的函数 `sendfile`。



<img src="pics/sendfile-传输.png" alt="sendfile-传输" style="zoom:67%;" />



只需要 1 个系统调用函数，即可完成文件传输操作，此时文件传输已经优化到 2 次上下文切换和 3 次数据拷贝。（但这还不是真正的零拷贝技术

**如果网卡支持 SG-DMA 技术**。



<img src="pics/零拷贝.webp" alt="零拷贝" style="zoom:67%;" />



SG-DMA 技术可以直接将数据从内存的内核缓冲区拷贝到网卡控制器缓冲区，不需要经过 Socket 缓冲区，但是数据的描述符和数据长度仍需要交由 Socket 缓冲区返回给用户进程获取信息。

这就是所谓的**零拷贝技术**，它做到了：

- **不在内存层面进行数据拷贝**（原本数据需要从内存的内核缓冲区拷贝到内存的 Socket 缓冲区
- **全程没有通过 CPU 来搬运数据**（原本从内存一部分拷贝到另一部分需要 CPU 来操作，而其它部分，从磁盘到内存，内存到网卡，都由 DMA 来进行

此技术对比传统文件传输，只需要 2 次上下文交换和 2 次数据拷贝操作，**整整提升了一倍以上的性能**。

#### 使用零拷贝技术的项目

- Kafka

  Kafka 进行文件传输时，使用 Java NIO 库里的 `transferTo` 方法，如果 Linux 版本支持 `sendfile` 方法，那么前一个方法最终就会使用后一个方法进行文件传输操作，使用零拷贝技术提升文件传输效率。

- Nginx

  Nginx 默认使用 `sendfile` 进行文件传输，如果关闭 `sendfile` 选项，将使用 `read` + `write` 进行传统的文件传输操作。

### PageCache 的作用

上述所提到的内核缓冲区其实就是磁盘高速缓存 PageCache，它的优点主要是两个：

- 缓存最近被访问的数据；
- 预读功能。

上述两个做法都是基于【局部性】原则，将大大提高读写磁盘的效率。

但是，PageCache 容量是有上限的，在**传输大文件**时，这些大文件数据会很快占满整个 PageCache ，这就会导致：

- 一些原本的**热点**数据被换出，随后又换入，影响磁盘读写性能；
- 大文件部分数据的被**再次**访问概率较低，无法享受缓存所带来的好处。

由于零拷贝技术使用了 PageCache 来提升性能，而传输大文件反而降低了性能，**因此传输大文件时，不应使用零拷贝技术**。

### 大文件传输

<img src="pics/大文件传输.png" alt="大文件传输" style="zoom:67%;" />



大文件传输应该使用【异步 I/O】结合【直接 I/O】的方式来替代零拷贝技术：

- 异步 I/O：用户进程发起异步 I/O 操作后，不需要【阻塞等待】，当内核将数据准备好时，会通知用户进程进行下一步操作；
- 直接 I/O：数据不经过 PageCache，而是直接从【磁盘控制器缓冲区】拷贝到【用户缓冲区】。

Nginx 通过设置 `directio` 参数，使得超过该值大小的文件使用异步 I/O 和直接 I/O 进行传输，否则使用零拷贝技术。

## I/O 多路复用

在 Linux 中使用 I/O 多路复用，最好搭配非阻塞 I/O 一起使用，这是因为在 Linux 系统下**多路复用的 API 返回的事件不一定可读写**，如果使用阻塞 I/O，对于通知可读写的事件却无法读写，将一直处于阻塞状态，而非阻塞 I/O 当事件无法读写时会立即返回。

### 为什么要使用 I/O 多路复用

如果不使用 I/O 多路复用，服务端为了支持多个客户端，比较传统的做法就是多进程 OR 多线程。

多进程的缺点在于扛不住高并发量，系统资源占用高且进程上下文切换的“包袱”很重。

多线程的缺点同样在于无法抗住高并发量（通常是并发量大于 1000 的场景），虽然减少了上下文切换的开销，但是有多少的并发量就必须开多少的线程，系统资源占用高的问题依旧存在。

既然为每一个请求分配一个进程 / 线程不合适，有没有办法通过一个进程来维护多个请求呢？有的，就是 I/O 多路复用技术。

可以理解为一个进程虽然任一时刻只能处理一个请求，但是处理每个请求的事件时，耗时控制在 1ms 以内，这样 1s 内就可以处理上千个请求，把时间拉长来看，多个请求复用了一个进程，这种思想类似 CPU 并发多个线程，所以也叫【时分多路复用】。

### select/poll

`select` 实现 I/O 多路复用的方式是先将已连接的 Socket 组织成一个文件描述符集合，然后按照如下流程进行：

1. 进程调用 select 函数，将文件描述符集合传递给内核；
2. 内核遍历集合，标记有事件发生的 Socekt；
3. 内核将文件描述符集合传递给用户态；
4. 用户态遍历集合，处理有事件发生的 Socket。

可以看到这样实现，需要遍历 2 次文件描述符集合，需要拷贝 2 次文件描述符集合。

另外，`select` 使用固定长度的 BitsMap 来记录文件描述符，因此能监听的 Socket 数量有上限，在 Linux 系统中，由内核中的 `FD_SETSIZE` 参数控制，默认最大值为 `1024`，只能监听 0~1023 的文件描述符。

`poll` 流程和 `select` 类似，但是 `poll` 使用链表来组织文件描述符集合，突破了上限，但是仍会受到系统文件描述符的限制。

总结：`select` 和 `poll` 类似，都是使用线性结构来组织进程关注的 Socket 集合，遍历时间复杂度为 O(n)，随着并发数的增加，性能下降得也就越明显。

### epoll

<img src="pics/epoll.webp" alt="epoll" style="zoom:67%;" />

`epoll` 通过两个方面解决了 `select` 和 `poll` 的问题：

- 在内核使用**红黑树**组织进程所关注的 Socket 文件描述符，添加一个文件描述符，或者删除一个文件描述符，或者修改一个文件描述符上的监听事件类型，都可以维持在一个非常稳定的查找性能；
- 当 Socket 上有事件触发时，会通过事先预设的**回调函数**将该文件描述符加入到**就绪事件链表**中，当用户调用 `epoll_wait()` 时，拿到的都是发生了事件的 Socket，不再需要遍历整个集合查看哪个 Socket 上触发了事件。

### select / poll vs epoll

既然 epoll 这么高效，那 select API 还有存在的必要吗？

**有的**，可以考虑低并发量的场景，比如在一个低并发量且这些 Socket 长期处于一个活跃状态的场景下。比如大型网络 FPS 游戏，此时需要长时间保持和每个客户端之间的连接，并且每个连接几乎一直都处于活跃状态，比如服务期间的心跳信息和数据同步信息。这个时候使用 epoll，使用红黑树和链表结构来维护这些 fd 属于”杀鸡焉用牛刀“的典例，性价比反而不高。当然如果处于高并发量的场景下，epoll 依旧是首选。

select 性能开销就开销在遍历和盲猜上，因此在并发量低且 Socket 都比较活跃的场景下，select 不见得比 epoll 慢，而且实现还更简单。

### 水平触发和边缘触发

- 水平触发：当内核通知文件描述符可读写时，后续会继续去检测其可读写状态，看它是否依然可读写并进行通知。所以用户态收到通知后，没必要一次性执行尽可能多的读写操作，可以继续调用比如 `epoll_wait` 函数来获取之前未读写完的文件描述符；
- 边缘触发：当文件描述符可读写时，内核只会通知一次，用户态在收到通知后，要尽可能地读写数据，以免错失数据读写的机会。会通过**循环**不断对文件描述符进行数据读写，如果文件描述符是阻塞的，进程也会阻塞在读写函数处，无法继续向下执行。所以，**边缘触发通常和非阻塞 I/O 配合使用**。

一句话：**水平触发只要满足事件的条件，比如内核有数据可读，内核就一直不断地将事件传递给用户；边缘触发则是第一次满足条件时才触发，后续不会传递同样的事件。**

一般来说，边缘触发比水平触发效率要高，因为减少了系统调用的次数，但是要求必须一次性将所有的数据处理完，增加了代码复杂度。

有一个场景适合边缘触发模式，比如接收一个数据包，由于种种原因使得数据包发送不完整，只能分批发送，于是服务端缓冲区里只有部分数据，如果直接取出，需要在进程内存中维护这部分数据，正确的做法应该是等待后续数据到达后将其补全并触发可读事件后，再一次性取出。如果是水平触发模式，就会因为缓冲区一直不为空而一直触发事件。

或者说在写大容量数据包的时候，如果采用水平触发，一旦缓冲区可写，就会一直触发可写事件，但此时并不能将数据包完全写入，如果只写入部分数据包还要考虑维护成本，如果采用边缘触发，可以等到缓冲区完全可写时，再触发一次可写事件对其进行写入。

上述提到的 `select` 和 `poll` 只有水平触发模式，`epoll` 默认也是水平触发模式，但是可以根据场景设置为边缘触发模式。

## Reactor 和 Proactor

### Reactor

通过操作系统所提供的 I/O 多路复用系统调用函数：

- 当没有事件发生时，进程 / 线程会阻塞在当前系统调用处，直到有事件发生；
- 有事件发生时，进程 / 线程会从阻塞态返回，并获取内核返回的产生了事件的连接集合，在用户态中对连接业务进行处理。

使用 I/O 多路复用接口编程，是以面向过程的方式，开发效率不高。后来基于面向对象的思想，**对 I/O 多路复用作了一层封装**，可以让开发者不需要关注底层网络 API 的细节，只需关注应用代码的编写。后来为这种模式取了一个名称叫做 Reactor 模式。

**Reactor 模式也叫 Dispatcher 模式：即 I/O 多路复用监听事件，收到事件后，根据事件类型分配给某个进程/线程。**

Reactor 模式主要由【Reactor】和【处理资源池】两个核心部分组成：

- Reactor 负责监听和分发事件；
- 处理资源池负责处理事件。

#### 单 Reactor 单进程/线程

<img src="pics/单reactor单进程.png" alt="单reactor单进程" style="zoom:67%;" />



- 优点：实现简单，不需要考虑进程间通信和多进程竞争。
- 缺点：无法充分利用多核 CPU 的性能，Handler 需避免处理长业务，否则会造成后续业务的延时。
- 场景：适用于短业务场景，不适合计算密集型业务。
- 应用：**Redis 采用单 Reactor 单进程的方案，因为 Redis 是面向内存进行操作，速度很快，性能瓶颈并不在 CPU 上。**

#### 单 Reactor 多线程

<img src="pics/单reactor多线程.png" alt="单reactor多线程" style="zoom:67%;" />



**和单 Reactor 单进程/线程的区别在于，Handler 不再负责业务处理，只负责数据的接收和发送，业务处理交给子线程中的 Processor 对象进行。**

- 优点：享受多核 CPU 所带来的性能提升；
- 缺点：由于连接读写事件仍由 Reactor 来负责，因此多个 Handler 进行写操作时，会面临单个 Reactor 对象上的共享数据竞争问题。

#### 多 Reactor 多进程/线程

<img src="pics/多reactor多进程.png" alt="多reactor多进程" style="zoom:67%;" />



- 优点：主线程和子线程分工明确，主线程建立连接，子线程业务处理，子线程不需要返回数据给主线程，**主线程将连接交给子线程后**，子线程可以自己和客户端进行数据传输；
- 应用：Netty 和 Memcache 采用了【多 Reactor 多线程】方案，Nginx 采用了【多 Reactor 多进程】方案。

### Proactor

**Reactor 是非阻塞同步网络模式，Proactor 是异步网络模式。**

异步指的是 I/O 过程中涉及到的【内核将数据准备好】和【数据从内核态拷贝到用户态】这两个过程都不需要等待。

- Reactor 是非阻塞同步网络模式，感知的是【已就绪】可读写事件；
- Proactor 是异步网络模式，感知的是【已完成】的读写事件。



<img src="pics/proactor.png" alt="proactor" style="zoom:67%;" />

